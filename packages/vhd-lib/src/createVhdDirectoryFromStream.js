import { VhdDirectory } from './'
import { BLOCK_UNUSED, FOOTER_SIZE, HEADER_SIZE, SECTOR_SIZE } from './_constants'
import { readChunk } from '@vates/read-chunk'
import assert from 'assert'
import { Disposable } from 'promise-toolbox'
import { unpackFooter, unpackHeader, computeBlockBitmapSize } from './Vhd/_utils'
import { asyncEach } from '@vates/async-each'

const cappedBufferConcat = (buffers, maxSize) => {
  let buffer = Buffer.concat(buffers)
  if (buffer.length > maxSize) {
    buffer = buffer.slice(buffer.length - maxSize)
  }
  return buffer
}

async function* parse(stream) {
  let bytesRead = 0

  // handle empty space between elements
  // ensure we read stream in order
  async function read(offset, size) {
    assert(bytesRead <= offset, `offset is ${offset} but we already read ${bytesRead} bytes`)
    if (bytesRead < offset) {
      // empty spaces
      await read(bytesRead, offset - bytesRead)
    }
    const buf = await readChunk(stream, size)
    assert.strictEqual(buf.length, size, `read ${buf.length} instead of ${size}`)
    bytesRead += size
    return buf
  }

  const bufFooter = await read(0, FOOTER_SIZE)

  const footer = unpackFooter(bufFooter)
  yield { type: 'footer', footer, offset: 0 }

  const bufHeader = await read(FOOTER_SIZE, HEADER_SIZE)
  const header = unpackHeader(bufHeader, footer)

  yield { type: 'header', header, offset: SECTOR_SIZE }
  const blockSize = header.blockSize
  assert.strictEqual(blockSize % SECTOR_SIZE, 0)

  const blockBitmapSize = computeBlockBitmapSize(blockSize)
  const blockAndBitmapSize = blockBitmapSize + blockSize

  const index = []

  for (const parentLocatorId in header.parentLocatorEntry) {
    const parentLocatorEntry = header.parentLocatorEntry[parentLocatorId]
    // empty parent locator entry, does not exist in the content
    if (parentLocatorEntry.platformDataSpace === 0) {
      continue
    }
    index.push({
      ...parentLocatorEntry,
      type: 'parentLocator',
      offset: parentLocatorEntry.platformDataOffset,
      size: parentLocatorEntry.platformDataLength,
      id: parentLocatorId,
    })
  }

  const batOffset = header.tableOffset
  const batSize = Math.max(1, Math.ceil((header.maxTableEntries * 4) / SECTOR_SIZE)) * SECTOR_SIZE

  index.push({
    type: 'bat',
    offset: batOffset,
    size: batSize,
  })

  // sometimes some parent locator are before the BAT
  index.sort((a, b) => a.offset - b.offset)
  while (index.length > 0) {
    const item = index.shift()
    const buffer = await read(item.offset, item.size)
    if (item.type === 'bat') {
      // found the BAT : read it and ad block to index
      for (let blockCounter = 0; blockCounter < header.maxTableEntries; blockCounter++) {
        const batEntrySector = buffer.readUInt32BE(blockCounter * 4)
        // unallocated block, no need to export it
        if (batEntrySector !== BLOCK_UNUSED) {
          const batEntryBytes = batEntrySector * SECTOR_SIZE
          // ensure the block is not before the bat
          assert.ok(batEntryBytes >= batOffset + batSize)
          index.push({
            type: 'block',
            id: blockCounter,
            offset: batEntryBytes,
            size: blockAndBitmapSize,
          })
        }
      }
      // sort again index to ensure block and parent locator are in the right order
      index.sort((a, b) => a.offset - b.offset)
    } else {
      yield { ...item, buffer }
    }
  }

  /**
   * the second footer is at filesize - 512 , there can be empty spaces between last block
   * and the start of the footer
   *
   * we read till the end of the stream, and use the last 512 bytes as the footer
   */
  const bufFooterEnd = await readLastSector(stream)
  assert(bufFooter.equals(bufFooterEnd), 'footer1 !== footer2')
}

function readLastSector(stream) {
  return new Promise((resolve, reject) => {
    let bufFooterEnd = Buffer.alloc(0)
    stream.on('data', chunk => {
      if (chunk.length > 0) {
        bufFooterEnd = cappedBufferConcat([bufFooterEnd, chunk], SECTOR_SIZE)
      }
    })

    stream.on('end', () => resolve(bufFooterEnd))
    stream.on('error', reject)
  })
}

const buildVhd = Disposable.wrap(async function* (handler, path, inputStream, { concurrency }) {
  const vhd = yield VhdDirectory.create(handler, path)
  await asyncEach(
    parse(inputStream),
    async function (item) {
      switch (item.type) {
        case 'footer':
          vhd.footer = item.footer
          break
        case 'header':
          vhd.header = item.header
          break
        case 'parentLocator':
          await vhd.writeParentLocator({ ...item, data: item.buffer })
          break
        case 'block':
          await vhd.writeEntireBlock(item)
          break
        default:
          throw new Error(`unhandled type of block generated by parser : ${item.type} while generating ${path}`)
      }
    },
    {
      concurrency,
    }
  )
  await Promise.all([vhd.writeFooter(), vhd.writeHeader(), vhd.writeBlockAllocationTable()])
})

export async function createVhdDirectoryFromStream(handler, path, inputStream, { validator, concurrency = 16 } = {}) {
  try {
    await buildVhd(handler, path, inputStream, { concurrency })
    if (validator !== undefined) {
      await validator.call(this, path)
    }
  } catch (error) {
    // cleanup on error
    await handler.rmTree(path)
    throw error
  }
}
